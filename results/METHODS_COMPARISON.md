# Methods Comparison

This document summarizes Baselines and LLM methods evaluated on the same 100 test pairs. Use the consolidated artifacts below for the full view (including Stage 1/2, ensembles, and calibration):

- Consolidated CSV: `comparison_consolidated.csv`
- Consolidated JSON: `comparison_consolidated.json`
- Baselines (with calibration): `baseline_comparison_v2.json`
- LLM Observer Stage 1: `llm_score_evaluation_stage1.json`
- LLM Observer Stage 2: `llm_score_evaluation_stage2.json`
- LLM Observer Stage 2 (calibrated from Stage 1): `llm_score_evaluation_stage2_calibrated.json`
- LLM Ensembles Stage 1: `ensemble_evaluation_stage1.json`
- LLM Ensembles Stage 2: `ensemble_evaluation_stage2.json`

To regenerate the consolidated report, run the generator from the `test` folder:

```
python experiments/generate_consolidated_report.py
```

This will refresh `comparison_consolidated.csv` and `comparison_consolidated.json`, and print a small leaderboard per stage.
# Complete Methods Comparison: All Models in One Place

**Generated:** November 4, 2025  
**Dataset:** 100 pairs (30% train, 70% test stratified split)  
**Feature Parity:** âœ… **Strict parity enforced** â€” All methods at each stage use identical input features  
**LLM Scaling:** âœ… **Optimal kx+b tuning on 30% train** â€” Same train/test protocol as baselines

---

## ðŸ“Š Quick Summary

### Best Performers by Metric

| Metric | Winner | Score | Method Type | Stage |
|--------|--------|-------|-------------|-------|
| **Match F1** | XGBoost V1 | 0.667 | Baseline | Time1 |
| **Match ROC AUC** | Random Forest V2 | 0.636 | Baseline | Time1+Time2 |
| **Match PR AUC** | Logistic V2 | 0.680 | Baseline | Time1+Time2 |
| **Like Correlation P1** | LLM Participant Stage2 | 0.159 | LLM (scores) | Time1+Time2 |
| **Like Correlation P2** | LLM Participant Stage2 | 0.110 | LLM (scores) | Time1+Time2 |

### Key Findings

1. **âœ… Feature Parity Confirmed:** All baselines use same features as LLM:
   - **V1/Stage1:** Time 1 only (pre-date profiles)
   - **V2/Stage2:** Time 1 + Time 2 (profiles + post-date reflections)

2. **âœ… LLM Scaling Applied:** All LLM methods use **optimal linear scaling** (kx+b) trained on 30% and tested on 70%:
   - Stage 1 Observer: k=-1.35, b=1.66 (F1=0.545) â€” **negative k confirms inverse correlation!**
   - Stage 2 Participant: k=0.06, b=0.54 (F1=0.641)
   - Stage 2 Observer: k=-0.12, b=0.64 (F1=0.641)
   - Stage 2 Advanced Observer: k=-0.16, b=0.66 (F1=0.641)

3. **ðŸŽ¯ Stage 2 > Stage 1 for LLM:** Adding Time 2 reflections improved LLM performance (Stage 2 F1=0.641 vs Stage 1 F1=0.545)

4. **ðŸ”„ Inverse Correlation Persists:** Observer methods still show **negative k** (higher LLM score â†’ lower match probability)

---

## ðŸ“‹ Complete Results Table

See **[comparison_consolidated.csv](comparison_consolidated.csv)** for raw data.

### Baseline Methods (8 total)

#### Stage 1: Time 1 Only (Pre-Date)

| Method | Accuracy | Precision | Recall | F1 | ROC AUC | PR AUC |
|--------|----------|-----------|--------|-------|---------|---------|
| **Similarity V1** | 0.514 | 0.514 | 0.543 | **0.528** | 0.536 | 0.576 |
| **Logistic V1** | 0.557 | 0.571 | 0.457 | **0.508** | 0.592 | 0.627 |
| **Random Forest V1** | 0.500 | 0.500 | 0.543 | **0.521** | 0.557 | 0.560 |
| **XGBoost V1** | 0.500 | 0.500 | **1.000** | **0.667** | 0.500 | 0.500 |

#### Stage 2: Time 1 + Time 2 (Post-Date Reflections)

| Method | Accuracy | Precision | Recall | F1 | ROC AUC | PR AUC |
|--------|----------|-----------|--------|-------|---------|---------|
| **Similarity V2** | 0.500 | 0.500 | 0.571 | **0.533** | 0.500 | 0.548 |
| **Logistic V2** | 0.600 | 0.613 | 0.543 | **0.576** | 0.609 | **0.680** |
| **Random Forest V2** | 0.586 | 0.575 | 0.657 | **0.613** | **0.636** | 0.664 |
| **XGBoost V2** | 0.586 | 0.583 | 0.600 | **0.592** | 0.607 | 0.640 |

**Feature Details:**
- **V1:** Age, gender, race, career, field_of_study, 17 interests, self-ratings (attractiveness, sincerity, intelligence, fun, ambition), partner preferences (6 traits)
- **V2:** V1 features + Time 2 updated self-ratings, Time 2 updated partner preferences, satisfaction scores

---

### LLM Methods with Optimal Scaling (6 total)

#### Stage 1: Time 1 Only (Immediate Evaluation)

| Method | Scaling (k, b) | Accuracy | Precision | Recall | F1 | ROC AUC | PR AUC |
|--------|----------------|----------|-----------|--------|-------|---------|---------|
| **Observer Stage1** | k=-1.35, b=1.66 | 0.500 | 0.477 | 0.636 | **0.545** | 0.403 | 0.421 |

**Notes:**
- Only Observer ran for Stage 1 (participant scores empty in stage1 file)
- **Negative k=-1.35** confirms inverse correlation: higher LLM score â†’ lower match probability
- Uses 0-10 LLM scores scaled linearly to match probabilities

**Input:** Persona narratives (Time 1) + conversation transcripts

---

#### Stage 2: Time 1 + Time 2 (With Reflection)

| Method | Scaling (k, b) | Accuracy | Precision | Recall | F1 | ROC AUC | PR AUC |
|--------|----------------|----------|-----------|--------|-------|---------|---------|
| **Participant Stage2** | k=0.06, b=0.54 | 0.471 | 0.471 | **1.000** | **0.641** | 0.524 | 0.542 |
| **Observer Stage2** | k=-0.12, b=0.64 | 0.471 | 0.471 | **1.000** | **0.641** | 0.487 | 0.469 |
| **Advanced Observer Stage2** | k=-0.16, b=0.66 | 0.471 | 0.471 | **1.000** | **0.641** | 0.466 | 0.466 |

**Notes:**
- All Stage 2 methods show identical F1 (0.641) because they all predict perfect recall (1.0)
- **Observer methods still show negative k**, though smaller magnitude than Stage 1
- Advanced Observer (with in-context learning) doesn't improve performance

**Input:** Persona narratives (Time 1 + Time 2 reflections) + conversation transcripts

**Reflection Context Includes:**
- Post-date satisfaction (1-10 scale)
- Updated partner preferences (100-point allocation across 6 traits)
- Updated self-ratings (1-10 scale on 5 traits)
- Updated perception of how others see me (1-10 scale on 5 traits)
- Temporal changes ("I now rate myself X compared to Y before the date")

---

### Ensemble Methods (4 total)

Built on **Stage 1 LLM scores** (participant + observer):

| Method | Accuracy | Precision | Recall | F1 | ROC AUC | PR AUC |
|--------|----------|-----------|--------|-------|---------|---------|
| **Participant Combined (individual)** | 0.557 | 0.542 | 0.743 | **0.627** | 0.556 | 0.539 |
| **Observer Normalized (individual)** | 0.500 | 0.500 | 0.971 | **0.660** | 0.506 | 0.521 |
| **Linear Regression** | 0.571 | 0.586 | 0.486 | **0.531** | 0.498 | 0.516 |
| **Logistic Regression** | 0.457 | 0.467 | 0.600 | **0.525** | 0.449 | 0.489 |

**Notes:**
- Individual observer outperforms ensembles (F1=0.660)
- Linear/logistic ensembles don't improve over individual methods
- Uses raw participant combined score (product) + observer normalized score (0-1)

---

### Like Prediction (Bonus Task)

Correlations between **LLM predicted like scores** (0-10) and **ground truth like ratings** (0-10):

| Method | Stage | Corr P1 | Corr P2 | MAE | RMSE | N |
|--------|-------|---------|---------|-----|------|---|
| **LLM Participant Stage2** | Time1+Time2 | **0.159** | **0.110** | 1.502 | 1.881 | 100 |

**Notes:**
- Only Stage 2 has like predictions (Stage 1 participant scores empty)
- Weak positive correlations (r~0.1-0.16)
- Average error of 1.5 points on 0-10 scale
- Uses raw LLM scores (0-10) vs ground truth (0-10), no scaling needed

---

## ðŸ” Methodology Details

### Feature Parity Enforcement

**Stage 1 (V1 / Time 1 only):**
- âœ… **Baselines V1:** Extract raw Time 1 fields only (age, gender, interests, self-ratings, preferences)
- âœ… **LLM Stage 1:** Persona narratives generated from Time 1 fields only
- âœ… **Ensembles:** Use Stage 1 LLM scores as input

**Stage 2 (V2 / Time 1 + Time 2):**
- âœ… **Baselines V2:** V1 features + Time 2 numeric updates (updated self-ratings, updated preferences, satisfaction)
- âœ… **LLM Stage 2:** Persona narratives include Time 2 reflections with temporal changes + numeric ratings
- âœ… **No ensembles for Stage 2 yet** (future work)

### Train/Test Split Protocol

**All methods use identical stratified 30/70 split:**
1. **Source:** 100 pairs from `personas.json` (not CSV)
2. **Stratification:** 50 matches, 50 non-matches preserved in train/test
3. **Random seed:** 42 (reproducible)
4. **Train set:** 30 pairs (15 matches, 15 non-matches)
5. **Test set:** 70 pairs (35 matches, 35 non-matches)

**Baseline protocol:**
- Fit imputers/scalers **only on train set**
- Similarity thresholds tuned on **train set only**
- ML models trained on **train set only**
- All metrics reported on **test set (70%)**

**LLM protocol (NEW âœ…):**
- Find optimal kx+b scaling on **train set (30%)**
- Apply scaling to **test set (70%)**
- Report all metrics on **test set**

### LLM Scaling Details

**Optimal Linear Scaling:** `match_probability = k Ã— llm_score + b`

**Training procedure:**
1. Extract train pairs (30%) and their ground truth labels
2. Extract test pairs (70%) and their ground truth labels
3. Get LLM scores (0-1 for participant combined, 0-1 for observer normalized)
4. Use least-squares regression on train set: `GT_train = k Ã— LLM_train + b`
5. Apply to test set: `pred_test = k Ã— LLM_test + b`
6. Threshold at 0.5 for binary predictions
7. Compute all metrics on test set

**Why negative k for observers?**
- Higher LLM compatibility score â†’ Lower human match probability
- Example Stage 1: k=-1.35 means: `match_prob = -1.35 Ã— score + 1.66`
- If LLM says 1.0 (perfect match) â†’ probability = 0.31 (no match!)
- Suggests LLMs evaluate compatibility through fundamentally different criteria

---

## ðŸ“ˆ Visualizations

See **[comparison_consolidated.png](comparison_consolidated.png)** for:
- **Top panel:** Match F1 across all methods (grouped by stage)
- **Bottom panel:** Like correlations for Stage 1 & Stage 2 (Person 1 & 2)

---

## ðŸŽ¯ Recommendations

### For Match Prediction (Binary Classification)

1. **Best overall:** XGBoost V1 (F1=0.667, Time 1 only)
   - Simplest features, best performance
   - No need for post-date reflections

2. **Best ROC AUC:** Random Forest V2 (AUC=0.636)
   - Time 2 reflections help RF
   - Good for ranking pairs by match probability

3. **LLM methods:** Stage 2 Participant (F1=0.641 after scaling)
   - Competitive with baselines after optimal tuning
   - Uses natural language understanding
   - Perfect recall (1.0) but lower precision

### For Like Prediction (Regression)

1. **Only option:** LLM Participant Stage 2 (r~0.1-0.16)
   - Weak but positive correlations
   - Average error 1.5 points on 0-10 scale
   - Room for improvement

### For Future Work

1. **Rerun Stage 1 with participant scores** â€” Current Stage 1 only has observer
2. **Train Stage 2 ensembles** â€” Combine Stage 2 participant + observer + advanced
3. **Investigate inverse correlation** â€” Why do observers predict oppositely?
4. **Try non-linear scaling** â€” Maybe sigmoid or polynomial fits work better
5. **Analyze scaling factors** â€” What do k and b tell us about LLM behavior?

---

## ðŸ“ Files Reference

| File | Description |
|------|-------------|
| `comparison_consolidated.csv` | All metrics in clean CSV format |
| `comparison_consolidated.png` | Single figure with match F1 + like correlations |
| `baseline_comparison_v2.json` | Baseline results (30/70 split) |
| `llm_score_evaluation_stage1.json` | LLM Stage 1 results (observer only) |
| `llm_score_evaluation_stage2.json` | LLM Stage 2 results (participant + observer + advanced) |
| `ensemble_evaluation_stage1_30_70.json` | Ensemble results on Stage 1 scores |
| `personas.json` | 100 pairs with Time 1 + Time 2 data |
| `conversations.json` | Simulated conversations + ground truth |

---

## ðŸ”¬ Technical Notes

### Dependencies
```
Python >= 3.8
numpy >= 1.24.0
pandas >= 2.0.0
scikit-learn >= 1.3.0
matplotlib >= 3.7.0
```

### Reproducibility

All results are reproducible with:
```bash
# Baselines (30/70 split from personas)
python experiments/baseline_models_v2.py \
  --personas results/personas.json \
  --output-dir results

# LLM Stage 1 (observer only)
python experiments/llm_score_evaluator.py \
  --conversations results/conversations.json \
  --output-dir results \
  --stage 1 \
  --max-pair-workers 10 \
  --method observer \
  --observer-model "google/gemini-2.0-flash-exp:free"

# LLM Stage 2 (participant + observer + advanced)
python experiments/llm_score_evaluator.py \
  --conversations results/conversations.json \
  --output-dir results \
  --stage 2 \
  --icl-examples results/icl_examples.json \
  --max-pair-workers 10 \
  --method both

# Ensembles (Stage 1 scores)
python experiments/ensemble_model.py \
  --llm-results results/llm_score_evaluation_stage1.json \
  --output results/ensemble_evaluation_stage1_30_70.json

# Consolidated comparison (with optimal scaling)
python experiments/consolidated_comparison.py
```

---

## ðŸ“Š Summary Statistics

- **Total methods evaluated:** 18 (8 baselines + 4 LLM + 4 ensembles + 2 like prediction)
- **Total pairs:** 100 (50 matches, 50 non-matches)
- **Train set:** 30 pairs (30%)
- **Test set:** 70 pairs (70%)
- **Feature parity:** âœ… Strict (V1=Time1, V2=Time1+Time2)
- **LLM scaling:** âœ… Optimal kx+b on train (30%)
- **Best match F1:** 0.667 (XGBoost V1)
- **Best ROC AUC:** 0.636 (Random Forest V2)
- **Best like corr:** 0.159 (LLM Participant Stage2)

---

**Last Updated:** November 4, 2025  
**Status:** âœ… Complete â€” All methods tuned on 30%, tested on 70%, with strict feature parity
