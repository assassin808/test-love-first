# Speed Dating Compatibility Prediction - Experimental Pipeline

**Project:** LLM-based vs Traditional ML Compatibility Prediction  
**Dataset:** Speed Dating Experiment (Fisman & Iyengar, 2006)  
**Date:** November 2025  
**Final Test Set:** 100 pairs (50 matches, 50 non-matches)

---

## üìã Table of Contents
1. [Overview](#overview)
2. [Data Pipeline](#data-pipeline)
3. [Method Categories](#method-categories)
4. [Experimental Pipeline](#experimental-pipeline)
5. [Evaluation Framework](#evaluation-framework)
6. [File Structure](#file-structure)
7. [Reproduction Guide](#reproduction-guide)

---

## üéØ Overview

### Research Question
Can LLM-based methods predict romantic compatibility better than traditional machine learning approaches?

### Key Innovation
1. **Persona Generation:** Create rich narrative profiles from structured dating data
2. **LLM Conversation Simulation:** Simulate speed dating conversations
3. **Multi-Method Evaluation:** Compare LLM predictions with traditional ML baselines
4. **Ground Truth Evaluation:** Use actual 'like' scores (0-10) from participants

### Methodology Highlights
- **Train/Test Split:** Train on different participants, test on same 100 pairs
- **No Data Leakage:** LLM and baselines use equivalent information (Time 1 only)
- **Linear Scaling:** Fair comparison across different prediction scales
- **Multiple Metrics:** Regression (Pearson r) and Classification (F1, ROC AUC)

---

## üìä Data Pipeline

### Stage 1: Data Preprocessing
**Script:** `experiments/data_preprocessing.py`

**Input:** `Speed Dating Data.csv` (8,378 records, 551 participants)

**Process:**
1. Load raw speed dating data
2. Clean and validate fields
3. Extract participant attributes:
   - Demographics: age, gender, race, field of study, career
   - Interests: sports, tvsports, exercise, dining, museums, art, hiking, gaming, clubbing, reading, tv, theater, movies, concerts, music, shopping, yoga
   - Self-ratings: attractive, sincere, intelligent, fun, ambitious
   - Partner preferences: attr1_1, sinc1_1, intel1_1, fun1_1, amb1_1, shar1_1
4. Extract interaction outcomes:
   - Match decisions (binary)
   - Like ratings (0-10)
   - Probability ratings

**Output:** Cleaned dataset ready for persona generation

### Stage 2: Persona Generation
**Script:** `experiments/persona_generator.py`

**Input:** 
- Cleaned dating data
- Test set specification: 100 pairs

**Process:**
1. **Select Test Set:**
   - Choose 100 diverse pairs from dataset
   - Ensure balanced outcomes (50% matches)
   - Track unique participants (145 IDs)

2. **Generate Rich Personas:**
   - Convert structured data ‚Üí narrative descriptions
   - Create realistic backstories based on demographics
   - Include authentic interests and preferences
   - Generate system prompts for LLM simulation

3. **Add Ground Truth:**
   - Include actual match outcomes
   - Store 'like' ratings from both persons
   - Preserve original participant IDs (iid)

**Output:** `results/personas.json`
```json
{
  "pair_id": "pair_001",
  "iid1": 467,
  "iid2": 492,
  "person1": {
    "iid": 467,
    "gender": "female",
    "age": 25,
    "persona_narrative": "...",
    "system_prompt": "...",
    "pre_event_data": {...},
    "time2_reflection": {...}
  },
  "person2": {...},
  "ground_truth": {
    "match": true,
    "person1_ratings": {"like": 8.0, ...},
    "person2_ratings": {"like": 7.0, ...}
  }
}
```

### Stage 3: Conversation Simulation
**Script:** `experiments/speed_dating_simulator.py`

**Input:** `results/personas.json` (100 pairs)

**Process:**
1. **Multi-Round Conversations:**
   - Default: 5 rounds per pair
   - Each person takes turns speaking
   - LLM maintains character consistency
   - Natural conversation flow

2. **LLM Configuration:**
   - Model: GPT-4o-mini
   - Temperature: 1.0 (high diversity)
   - Repetition penalty: 1.15 (reduce monotony)
   - Max tokens: 150 per turn

3. **Quality Control:**
   - Track conversation quality
   - Handle API errors gracefully
   - Save checkpoints every 10 pairs

**Output:** `results/conversations.json`
```json
{
  "pair_id": "pair_467_492",
  "person1_id": 467,
  "person2_id": 492,
  "conversation": [
    {"speaker": "person1", "message": "Hi! I'm...", "round": 1},
    {"speaker": "person2", "message": "Nice to meet you...", "round": 1},
    ...
  ],
  "ground_truth": {...}
}
```

---

## üî¨ Method Categories

### Category 1: LLM-Based Methods

#### Method 1.1: Participant Self-Evaluation
**Approach:** Ask each LLM persona if they would date the other person

**Prompt:**
```
Based on your conversation, would you want to go on a second date with [name]?
Answer: Yes/No
Rating (0-10): [score]
```

**Score Calculation:**
- Individual scores: person1_score, person2_score (0-10)
- Combined score: (score1 √ó score2) / 100
- Binary prediction: both say "Yes" ‚Üí Match

**Characteristics:**
- ‚úÖ Considers both perspectives
- ‚úÖ Captures mutual interest
- ‚ö†Ô∏è May be overly optimistic

#### Method 1.2: Observer Evaluation
**Approach:** Third-party LLM observer evaluates compatibility

**Prompt:**
```
You are an expert dating coach. Based on this conversation between [person1] and [person2], 
rate their compatibility on a scale of 0-10.

Consider:
- Conversation flow and engagement
- Shared interests and values
- Chemistry and mutual interest
- Communication style compatibility

Rating (0-10): [score]
```

**Score Calculation:**
- Single compatibility score (0-10)
- Normalized to [0, 1]
- Binary: score ‚â• 5.0 ‚Üí Match

**Characteristics:**
- ‚úÖ Objective third-party view
- ‚úÖ Holistic compatibility assessment
- ‚ö†Ô∏è Surprising inverse relationship with human ratings!

#### Method 1.3: Advanced Observer (with ICL)
**Approach:** Observer method + In-Context Learning examples

**Enhancement:**
- Provide 10 exemplar pairs with known outcomes
- Show diverse examples (matches and non-matches)
- Include reasoning for each rating

**Process:**
1. Select 10 ICL examples with diverse characteristics
2. Include full conversation + ground truth ratings
3. Prepend examples to observer prompt
4. Evaluate test pairs with context

**Expected:** Better calibration with human judgments  
**Reality:** Performance decreased (Pearson 0.055 vs 0.200)

### Category 2: Ensemble Methods

#### Method 2.1: Linear Regression Ensemble
**Approach:** Combine 3 LLM scores via linear regression

**Features:**
- Participant combined score (0-1)
- Observer normalized score (0-1)
- Advanced observer score (0-1)

**Training:**
- Use same 100 pairs with ground truth
- 5-fold cross-validation
- Optimize for ROC AUC

**Model:** `like_combined = w1√óparticipant + w2√óobserver + w3√óadvanced_observer + b`

**Result:** Underperformed individual Observer method

#### Method 2.2: Logistic Regression Ensemble
**Approach:** Combine 3 LLM scores for binary classification

**Same features as Linear Ensemble**

**Training:**
- 5-fold CV with balanced class weights
- Optimize for ROC AUC
- Binary target: both_like ‚â• 5.0

**Result:** Also underperformed (likely due to contradictory signals)

### Category 3: Traditional ML Baselines

#### Method 3.1: Similarity V1 (Cosine Similarity)
**Features:** Time 1 only (pre-event data)
- Demographics: age, gender, race, field, career
- Interests: 17 interest ratings (sports, movies, etc.)
- Self-ratings: attractive, sincere, intelligent, fun, ambitious
- Preferences: 6 preference weights

**Approach:**
1. Create feature vector for each person (98 dimensions)
2. Compute cosine similarity between pairs
3. Threshold tuned on training data (no leakage!)

**Result:** Best F1 score (0.925) - surprisingly effective!

#### Method 3.2: Similarity V2 (with Time 2)
**Additional Features:** Post-event reflections
- Interest correlations with partner
- Updated preferences
- Activity preferences

**Total:** 126 dimensions

**Result:** Slightly worse than V1 (Time 2 adds noise)

#### Methods 3.3-3.8: ML Models (Logistic, Random Forest, XGBoost)
**V1 versions:** Time 1 only (same features as Similarity V1)  
**V2 versions:** Time 1 + Time 2 (same features as Similarity V2)

**Training:**
- 2,398 training pairs (different participants from test set)
- Balanced class weights to handle 16.3% match rate
- Threshold tuning on training data

**Models:**
- **Logistic Regression:** Linear baseline
- **Random Forest:** 200 trees, unlimited depth
- **XGBoost:** Gradient boosting with balanced classes

**Results:**
- Logistic: Best correlation with 'like' scores
- Random Forest: Failed completely (F1=0.0)
- XGBoost: Moderate performance

---

## üîÑ Experimental Pipeline

### Phase 1: Data Preparation
```bash
# 1. Preprocess raw data
python experiments/data_preprocessing.py

# 2. Generate personas for 100 test pairs
python experiments/persona_generator.py \
  --input "Speed Dating Data.csv" \
  --output results/personas.json \
  --num-pairs 100
```

### Phase 2: LLM Evaluation
```bash
# 3. Simulate conversations
python experiments/speed_dating_simulator.py \
  --pairs results/personas.json \
  --output-dir results \
  --num-rounds 5

# 4. Create ICL examples
python experiments/create_icl_examples.py \
  --conversations results/conversations.json \
  --personas results/personas.json \
  --output results/icl_examples.json \
  --num-examples 10

# 5. Evaluate LLM methods (participant, observer, advanced observer)
python experiments/llm_score_evaluator.py \
  --conversations results/conversations.json \
  --output-dir results \
  --method both \
  --icl-examples results/icl_examples.json
```

### Phase 3: Ensemble & Baseline Evaluation
```bash
# 6. Train ensemble models
python experiments/ensemble_model.py \
  --llm-results results/llm_score_evaluation.json \
  --output results/ensemble_evaluation.json

# 7. Train and evaluate baselines
python experiments/baseline_models_v2.py \
  --personas results/personas.json \
  --output-dir results
```

### Phase 4: Comprehensive Comparison
```bash
# 8. Evaluate all methods against 'like' scores
python experiments/evaluate_like_improved.py

# Output:
# - results/like_score_evaluation_improved.json
# - results/like_score_comparison_improved.png
```

---

## üìà Evaluation Framework

### Ground Truth: 'Like' Scores
- **Source:** Original speed dating dataset
- **Scale:** 0-10 (continuous rating)
- **Location:** `ground_truth.person1_ratings.like`, `person2_ratings.like`
- **Combined Score:** `(like1 √ó like2) / 100` ‚Üí normalized [0, 1]
- **Binary Classification:** `(like1 ‚â• 5.0 AND like2 ‚â• 5.0)` ‚Üí Match = 1

### Evaluation Metrics

#### Regression (Continuous 'like' scores)
1. **Pearson Correlation (r)**
   - Linear correlation between prediction and ground truth
   - Range: -1 to 1 (0 = no correlation)
   - **With Scaling:** Train `like_combined = k √ó pred + b` for fair comparison
   - **Without Scaling:** Direct correlation (unfair - different scales)

2. **Spearman Correlation (œÅ)**
   - Rank-based correlation (robust to outliers)
   - Better for non-linear relationships

3. **MSE / MAE**
   - Mean Squared Error / Mean Absolute Error
   - Lower is better

#### Classification (Binary match prediction)
1. **ROC AUC**
   - Area under Receiver Operating Characteristic curve
   - 0.5 = random, 1.0 = perfect
   - Measures ranking ability

2. **PR-AUC**
   - Precision-Recall Area Under Curve
   - Better for imbalanced datasets
   - Focuses on positive class (matches)

3. **F1 Score**
   - Harmonic mean of Precision and Recall
   - Balances false positives and false negatives
   - Range: 0 to 1

4. **Accuracy**
   - Simple correct/incorrect ratio
   - Less informative for imbalanced data

### Linear Scaling (Critical Innovation!)

**Problem:** Different methods use different scales
- Observer: 0-1 (normalized)
- Participant: 0-100 (product)
- Baselines: probability [0, 1]

**Solution:** Train linear transformation for each method
```python
like_combined = k √ó prediction + b
```

**Benefits:**
- Fair comparison across methods
- Reveals inverse relationships (negative k)
- True correlation with human judgments

**Discovery:** Some LLM methods have **negative k** (inverse relationship!)
- Observer: k = -0.3683
- Higher LLM score ‚Üí Lower human 'like' rating
- LLMs evaluate compatibility differently than humans!

---

## üìÅ File Structure

```
test/
‚îú‚îÄ‚îÄ Speed Dating Data.csv              # Raw dataset (8,378 records)
‚îú‚îÄ‚îÄ Speed Dating Data Key.txt          # Data dictionary
‚îÇ
‚îú‚îÄ‚îÄ experiments/                        # All experiment scripts
‚îÇ   ‚îú‚îÄ‚îÄ data_preprocessing.py          # [1] Clean raw data
‚îÇ   ‚îú‚îÄ‚îÄ persona_generator.py           # [2] Generate test personas
‚îÇ   ‚îú‚îÄ‚îÄ speed_dating_simulator.py      # [3] Simulate conversations
‚îÇ   ‚îú‚îÄ‚îÄ create_icl_examples.py         # [4] Create ICL examples
‚îÇ   ‚îú‚îÄ‚îÄ llm_score_evaluator.py         # [5] Evaluate LLM methods
‚îÇ   ‚îú‚îÄ‚îÄ ensemble_model.py              # [6] Train ensemble models
‚îÇ   ‚îú‚îÄ‚îÄ baseline_models_v2.py          # [7] Train baseline models
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_like_improved.py      # [8] Final comprehensive evaluation
‚îÇ
‚îú‚îÄ‚îÄ results/                            # All experiment outputs
‚îÇ   ‚îú‚îÄ‚îÄ personas.json                  # 100 test pairs with ground truth
‚îÇ   ‚îú‚îÄ‚îÄ conversations.json             # Simulated conversations
‚îÇ   ‚îú‚îÄ‚îÄ icl_examples.json              # 10 ICL exemplars
‚îÇ   ‚îú‚îÄ‚îÄ llm_score_evaluation.json      # LLM method results
‚îÇ   ‚îú‚îÄ‚îÄ ensemble_evaluation.json       # Ensemble method results
‚îÇ   ‚îú‚îÄ‚îÄ baseline_comparison_v2.json    # Baseline method results
‚îÇ   ‚îú‚îÄ‚îÄ like_score_evaluation_improved.json     # Final comprehensive results
‚îÇ   ‚îî‚îÄ‚îÄ like_score_comparison_improved.png      # Comparison plots
‚îÇ
‚îú‚îÄ‚îÄ EXPERIMENTAL_PIPELINE.md           # üìò This file - complete pipeline
‚îú‚îÄ‚îÄ FINAL_EVALUATION_SUMMARY.md        # üìä Final results and findings
‚îú‚îÄ‚îÄ EVALUATION_CLARIFICATIONS.md       # üìù Methodology Q&A
‚îî‚îÄ‚îÄ requirements.txt                    # Python dependencies
```

### Deprecated/Archived Files
- `baseline_models.py` - Old version (use baseline_models_v2.py)
- `evaluate_against_like_score.py` - No scaling (use evaluate_like_improved.py)
- `comprehensive_comparison.py` - Outdated comparison script
- Various test scripts (test_*.py, analyze_results.py, quick_start.py)
- Old documentation (BEFORE_AFTER_COMPARISON.md, TEMPERATURE_VERIFICATION.md, etc.)

---

## üîÅ Reproduction Guide

### Prerequisites
```bash
# Install dependencies
pip install -r requirements.txt

# Set up OpenAI API key
cp .env.example .env
# Edit .env and add your OPENAI_API_KEY
```

### Quick Start (Full Pipeline)
```bash
# Run entire pipeline (takes ~2-3 hours)
bash run_full_experiment.sh
```

### Step-by-Step Execution

#### Step 1: Generate Test Set
```bash
python experiments/persona_generator.py \
  --input "Speed Dating Data.csv" \
  --output results/personas.json \
  --num-pairs 100
```
**Output:** 100 pairs with rich personas  
**Time:** ~2 minutes

#### Step 2: Simulate Conversations
```bash
python experiments/speed_dating_simulator.py \
  --pairs results/personas.json \
  --output-dir results \
  --num-rounds 5 \
  --sample-size 100
```
**Output:** Conversations for 100 pairs (5 rounds each)  
**Time:** ~30-45 minutes (API calls)  
**Cost:** ~$2-3 (GPT-4o-mini)

#### Step 3: Create ICL Examples
```bash
python experiments/create_icl_examples.py \
  --conversations results/conversations.json \
  --personas results/personas.json \
  --output results/icl_examples.json \
  --num-examples 10
```
**Output:** 10 diverse examples for in-context learning  
**Time:** ~1 minute

#### Step 4: Evaluate LLM Methods
```bash
python experiments/llm_score_evaluator.py \
  --conversations results/conversations.json \
  --output-dir results \
  --method both \
  --icl-examples results/icl_examples.json \
  --max-pair-workers 5
```
**Output:** Participant, Observer, Advanced Observer results  
**Time:** ~20-30 minutes (API calls)  
**Cost:** ~$1-2

#### Step 5: Train Ensemble Models
```bash
python experiments/ensemble_model.py \
  --llm-results results/llm_score_evaluation.json \
  --output results/ensemble_evaluation.json
```
**Output:** Linear and Logistic Regression ensemble predictions  
**Time:** ~10 seconds

#### Step 6: Train Baseline Models
```bash
python experiments/baseline_models_v2.py \
  --personas results/personas.json \
  --output-dir results
```
**Output:** 8 baseline method results (Similarity, Logistic, RF, XGBoost √ó V1/V2)  
**Time:** ~30 seconds

#### Step 7: Comprehensive Evaluation
```bash
python experiments/evaluate_like_improved.py
```
**Output:** 
- `results/like_score_evaluation_improved.json` - All metrics
- `results/like_score_comparison_improved.png` - Comparison plots  
**Time:** ~5 seconds

### Viewing Results
```bash
# View final summary
cat FINAL_EVALUATION_SUMMARY.md

# View detailed metrics
python -c "
import json
with open('results/like_score_evaluation_improved.json') as f:
    results = json.load(f)
    print(json.dumps(results['summary'], indent=2))
"

# Open comparison plots
open results/like_score_comparison_improved.png
```

---

## üî¨ Key Technical Details

### Train/Test Split Strategy
- **Goal:** Test on same 100 pairs for all methods
- **Challenge:** Avoid data leakage from training set
- **Solution:**
  1. Identify 145 unique participants in test set
  2. Exclude ALL records involving these participants from training
  3. Train baselines on remaining 4,806 records (406 participants)
  4. LLM methods only use test pair information (no training data needed)

### Feature Parity
- **Principle:** LLM and baselines must use equivalent information
- **V1 Methods:** Only Time 1 data (pre-event)
  - Same information LLM personas have
  - Fair comparison
- **V2 Methods:** Time 1 + Time 2 (post-event reflections)
  - Unfair advantage (future knowledge)
  - Generally performed worse (noise > signal)

### Handling Inverse Relationships
- **Discovery:** Some methods predict OPPOSITE to human ratings
- **Impact:** Direct correlation appears weak
- **Solution:** Linear scaling reveals true relationship
  - Negative k indicates inverse relationship
  - After scaling: true correlation strength revealed
- **Interpretation:** LLM may evaluate compatibility differently than humans

### Threshold Tuning
- **Baselines:** Threshold tuned on training data only
- **No Leakage:** Test set never seen during threshold selection
- **Adaptive Strategies:** Random Forest & XGBoost use probability distribution
  - Mean probability
  - Median probability  
  - Top 50% (match expected test distribution)

---

## üìä Expected Results Summary

### Best Methods by Task

| Task | Best Method | Score |
|------|-------------|-------|
| Correlation with 'like' scores | Observer (LLM) | Pearson r = 0.20 |
| Binary match prediction | Similarity V1 (Baseline) | F1 = 0.92 |
| ROC AUC | Observer (LLM) | AUC = 0.58 |
| Raw correlation (no scaling) | Logistic V1 (Baseline) | Pearson r = 0.15 |

### Key Findings
1. **LLM Observer best at predicting human ratings** (with scaling)
2. **Simple cosine similarity best at binary classification**
3. **Ensemble methods underperformed** (contradictory signals)
4. **ICL decreased performance** (need better example selection)
5. **Time 2 data adds noise** (worse performance)
6. **Inverse relationships discovered** (LLM ‚â† human evaluation)

---

## üöÄ Future Directions

### Methodological Improvements
1. **Investigate inverse relationships** - Why do LLMs rate differently?
2. **Improve ICL strategy** - Better example selection and diversity
3. **Refine ensemble methods** - Handle contradictory signals
4. **Feature engineering for Time 2** - Extract signal from noise

### Experimental Extensions
1. **Longer conversations** - Test with 10+ rounds
2. **Different LLM models** - GPT-4, Claude, Llama
3. **Personality embeddings** - Use LLM embeddings as features
4. **Active learning** - Select most informative pairs

### Theoretical Questions
1. **What aspects of compatibility do LLMs capture?**
2. **Do inverse relationships reveal valid alternative perspectives?**
3. **Can we combine human and LLM judgments for better predictions?**
4. **How do conversation dynamics differ from structured features?**

---

## üìö References

### Dataset
Fisman, R., & Iyengar, S. (2006). Speed Dating Experiment Dataset. Columbia Business School.

### Related Work
- LLM-based personality assessment
- Conversational AI for relationship prediction
- Traditional ML for compatibility matching

---

## üìù Version History

- **v1.0 (Nov 2025):** Initial pipeline with 13 methods evaluated
- **Key Milestone:** Discovery of inverse relationships in LLM methods
- **Major Innovation:** Linear scaling for fair cross-method comparison

---

## ü§ù Contributing

For questions or improvements:
1. Check existing documentation first
2. Review experiment scripts for implementation details
3. Refer to FINAL_EVALUATION_SUMMARY.md for results interpretation

---

**Last Updated:** November 4, 2025  
**Status:** Complete - All 13 methods evaluated against ground truth
